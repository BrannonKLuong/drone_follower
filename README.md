Autonomous Tactical Scouting Drone
Status: In Progress - Hardware-in-the-Loop Validation

This repository contains the ROS 2 software stack for a compact, autonomous drone designed for tactical indoor and close-quarters operations. The system uses an offboard computing architecture to enable a small, agile aerial platform to perform complex perception and navigation tasks, controlled entirely by real-time hand gestures from an operator.

Live Demonstration
The core perception and control loop has been successfully validated in a software-in-the-loop (SITL) environment. The following demonstration shows the system in action:

On the left: The "Hand Gesture Recognition" window, processing a live video feed from a webcam. The AI model (MediaPipe) correctly identifies hand landmarks and classifies the gesture into a high-level command.

On the right: The RViz visualization window. The drone (represented by the TF axes) receives the command generated by the hand gesture and executes it, using its potential field algorithm to navigate the simulated maze environment.

(Insert your GIF here showing the gesture changing from "HOVER" to "GO_THROUGH_DOOR" and the drone moving through the maze)

Core Features & Capabilities
Real-Time Hand Gesture Control: The drone is controlled by high-level commands generated from an AI-powered perception node. The system uses a debouncing algorithm, requiring a gesture to be held for several frames before a command is confirmed, preventing accidental actions. Implemented gestures include:

Fist (HOVER): Commands the drone to stop its current task and hold its position.

Open Palm (RESUME_STROBE_FOLLOW): Commands the drone to follow its primary mission objective (a moving target in simulation).

Pointing Finger (GO_THROUGH_DOOR): Commands the drone to move forward.

Dynamic Obstacle Avoidance via Potential Fields: The drone uses a custom potential field algorithm for navigation. This method calculates a real-time trajectory based on two competing forces:

Attractive Force: A vector pulling the drone towards its current goal (e.g., a waypoint or a moving target).

Repulsive Force: A vector pushing the drone away from any nearby obstacles. This force is generated using real-time data from the obstacle_perception_node, allowing the drone to smoothly maneuver around objects without a pre-planned path.

Multi-State Control System: A state machine manages the drone's behavior, allowing it to seamlessly arbitrate between different modes, such as following a target, executing a manual command, or initiating a "lost target" search pattern.

System Architecture
This project uses a Remote Computing (or "Offboard Computing") architecture to maximize the drone's agility and flight time while enabling powerful AI processing.

On-Drone Hardware (The "Body"): A lightweight Raspberry Pi 4 acts as a "video bridge." Its sole purpose is to stream sensor data from an Intel RealSense D435i depth camera over a low-latency wireless data link.

Ground Station (The "Brain"): A powerful ground computer receives the sensor data and runs the entire ROS 2 autonomy stack, including the perception, navigation, and control nodes. It sends high-level flight commands back to the drone.

Software Node Breakdown
This system is composed of several custom ROS 2 nodes that work in concert.

fly_up_land.py (The Brain): The central command node. It subscribes to all sensor and command inputs and is responsible for making high-level decisions. It implements the core Potential Field algorithm, using the simplified threat data from the perception node to calculate repulsive forces.

hand_gesture_recognition_node.py (The Operator Interface): This node uses OpenCV and the MediaPipe AI model to process a video stream. It detects hand landmarks, classifies the gesture, and publishes the corresponding command string to the /hand_commands topic for the "Brain" to act upon.

obstacle_perception_node.py (The Eyes): This node subscribes to the raw PointCloud2 data from the depth sensor. It processes this complex 3D data to find the most immediate threat to the drone and publishes the stable 3D coordinates of that threat to the /detected_obstacle topic. This simplifies the world for the "Brain."

mock_px4.py (Simulated Flight Controller): In the simulation, this node mimics a real PX4 flight controller. It receives TrajectorySetpoint messages and publishes the drone's changing position and status. This is replaced by the real Cube Orange+ in the hardware phase.

simulated_depth_sensor_publisher.py (Simulated World): This node creates the virtual maze environment by publishing the location of walls and obstacles as PointCloud2 data. This is replaced by the real Intel RealSense D435i in the hardware phase.

How to Run (Simulation)
The simulation is launched using the run_all.sh script, which automates the setup of the entire environment.

run_all.sh: This main script opens a series of terminals.

Terminal 1 (mock_px4): Starts the simulated flight controller.

Terminal 2 (simulated_depth_sensor): Starts the world generator, creating the maze.

Terminal 3 (RViz2): Launches the 3D visualizer.

Terminal 4 (obstacle_perception): Starts the perception node to process the simulated sensor data.

Terminal 5 (fly_up_land.py): Starts the main autonomy logic and decision-making brain.

Terminal 6 (Manual): To test with hand gestures, this terminal is used to launch the hand_gesture_recognition_node.py manually.

Current Status & Next Steps
The project is currently in the Hardware-in-the-Loop (HITL) validation phase. The core software has been successfully developed and stress-tested in simulation. The current active work involves:

Validating the Control Pipeline: Integrating a CubePilot Cube Orange+ flight controller with a Raspberry Pi 4. This involves sending commands from the ground station software and confirming they are correctly received and executed by the real hardware.

Validating the Perception Pipeline: Integrating the Intel RealSense D435i depth camera with the Raspberry Pi and successfully streaming real-time PointCloud2 and infrared data to the ground station.

The next major phase (Phase 2b) will be the integration of the validated electronics onto a 5-inch drone frame for physical flight testing.
